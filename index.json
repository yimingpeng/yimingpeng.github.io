[{"authors":null,"categories":null,"content":"","date":1547693428,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547693428,"objectID":"a0dda1b19bd052fd01ba9ef59a9e7cff","permalink":"yimingpeng.github.io/project/pes-project/","publishdate":"2019-01-17T15:50:28+13:00","relpermalink":"yimingpeng.github.io/project/pes-project/","section":"project","summary":"","tags":[],"title":"Proximal Evolutionary Strategy Project","type":"project"},{"authors":["\u003cb\u003eYiming Peng\u003c/b\u003e","Gang Chen","Mengjie Zhang"],"categories":null,"content":"","date":1547687956,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547687956,"objectID":"0fb49c4cffc5116f5ee9f04ab17a8b74","permalink":"yimingpeng.github.io/publication/journal/j-paper-1/","publishdate":"2019-01-17T14:19:16+13:00","relpermalink":"yimingpeng.github.io/publication/journal/j-paper-1/","section":"publication","summary":"","tags":[],"title":"Proximal Evolutionary Strategies: Achieving State-of-the-art Deep Reinforcement Learning through Evolutionary Policy Optimization","type":"publication"},{"authors":["Gang Chen","\u003cb\u003eYiming Peng\u003c/b\u003e","Mengjie Zhang"],"categories":null,"content":"","date":1542424019,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542424019,"objectID":"78bcb611abb3f2d2b78b40eb16f03ce1","permalink":"yimingpeng.github.io/publication/conference/c-paper-10/","publishdate":"2018-11-17T16:06:59+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-10/","section":"publication","summary":"Recently deep reinforcement learning (DRL) has achieved outstanding success on solving many difficult and large-scale RL problems. However the high sample cost required for effective learning often makes DRL unaffordable in resource- limited applications. With the aim of improving sample efficiency and learning performance, we will develop a new DRL algorithm in this paper that seamless integrates entropy-induced and bootstrap-induced techniques for efficient and deep exploration of the learning environment. Specifically, a general form of Tsallis entropy regularizer will be utilized to drive entropy-induced exploration based on efficient approximation of optimal action-selection policies. Different from many existing works that rely on action dithering strategies for exploration, our algorithm is efficient in exploring actions with clear exploration value. Meanwhile, by employing an ensemble of Q-networks under varied Tsallis entropy regularization, the diversity of the ensemble can be further enhanced to enable effective bootstrap-induced exploration. Experiments on Atari game playing tasks clearly demonstrate that our new algorithm can achieve more efficient and effective exploration for DRL, in comparison to recently proposed exploration methods including Bootstrapped Deep Q-Network and UCB Q-Ensemble.","tags":[],"title":"Effective Exploration for Deep Reinforcement Learning via Bootstrapped Q-Ensembles with Tsallis Entropy Regularization","type":"publication"},{"authors":["Gang Chen","\u003cb\u003eYiming Peng\u003c/b\u003e","Mengjie Zhang"],"categories":null,"content":"","date":1523934667,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523934667,"objectID":"fded0b65e797ff26b687e87e7427bb0e","permalink":"yimingpeng.github.io/publication/conference/c-paper-11/","publishdate":"2018-04-17T16:11:07+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-11/","section":"publication","summary":"Very recently proximal policy optimization (PPO) algorithms have been proposed as first-order optimization methods for effective reinforcement learn- ing. While PPO is inspired by the same learn- ing theory that justifies trust region policy optimization (TRPO), PPO substantially simplifies algorithm design and improves data efficiency by performing multiple epochs of clipped policy optimization from sampled data. Although clipping in PPO stands for an important new mechanism for efficient and reliable policy update, it may fail to adaptively improve learning performance in accordance with the importance of each sampled state. To address this issue, a new surrogate learning objective featuring an adaptive clipping mechanism is proposed in this paper, enabling us to develop a new algorithm, known as PPO-λ. PPO-λ optimizes policies repeatedly based on a theoretical tar- get for adaptive policy improvement. Meanwhile, destructively large policy update can be effectively prevented through both clipping and adaptive control of a hyperparameter λ in PPO-λ, ensuring high learning reliability. PPO-λ enjoys the same simple and efficient design as PPO. Empirically on several Atari game playing tasks and benchmark control tasks, PPO-λ also achieved clearly better performance than PPO.","tags":[],"title":"An Adaptive Clipping Approach for Proximal Policy Optimization","type":"publication"},{"authors":["Gang Chen","\u003cb\u003eYiming Peng\u003c/b\u003e","Mengjie Zhang"],"categories":null,"content":"","date":1516158206,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516158206,"objectID":"4697f50778ccd44da6ed7d2498cb7aba","permalink":"yimingpeng.github.io/publication/conference/c-paper-9/","publishdate":"2018-01-17T16:03:26+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-9/","section":"publication","summary":"Recent advancement on reinforcement learning (RL) algorithms shows that effective learning of parametric action- selection policies can often be achieved through direct optimization of a performance lower bound subject to pre-defined policy behavioral constraints. Driven by this understanding, this paper seeks to develop new policy search techniques where RL is achieved through maximizing a performance lower bound obtained originally based on an Expectation-Maximization method. For reliable RL, our new learning techniques must also simultaneously guarantee constrained policy behavioral changes measured through KL divergence. Two separate approaches will be pursued to tackle our constrained policy optimization problems, resulting in two new RL algorithms. The first algorithm utilizes a conjugate gradient technique and a Bayesian learning method for approximate optimization. The second algorithm focuses on minimizing a loss function derived from solving the Lagrangian for constrained policy search. Both algorithms have been experimentally examined on several benchmark problems provided by OpenAI GYM. The experiment results clearly demonstrate that our algorithms can be highly effective in comparison to several well-known RL algorithms.","tags":[],"title":"Constrained Expectation-Maximization Methods for Effective Reinforcement Learning","type":"publication"},{"authors":["\u003cb\u003eYiming Peng\u003c/b\u003e","Gang Chen","Harman Singh","Mengjie Zhang"],"categories":null,"content":"","date":1516156231,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516156231,"objectID":"5448a64f967b54a5808be353a5e447c2","permalink":"yimingpeng.github.io/publication/conference/c-paper-6/","publishdate":"2018-01-17T15:30:31+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-6/","section":"publication","summary":"NeuroEvolution of Augmenting Topology (NEAT) is one of the most successful algorithms for solving traditional reinforcement learning (RL) tasks such as pole-balancing. However, the algorithm faces serious challenges while tackling problems with large state spaces, particularly the Atari game playing tasks. This is due to the major flaw that NEAT aims at evolving a single neural network (NN) that must be able to simultaneously extract high-level state features and select action outputs. However such complicated NNs cannot be easily evolved directly through NEAT. To address this issue, we propose a new reinforcement learning scheme based on NEAT with two key technical advancements: (1) a new three-stage learning scheme is introduced to clearly separate feature learning and policy learning to allow effective knowledge sharing and learning across multiple agents; (2) various policy gradient search algorithms can be seamlessly integrated with NEAT for training policy networks with deep structures to achieve effective and sample efficient RL. Experiments on several Atari games confirm that our new learning scheme can be more effective and has higher sample efficiency than NEAT and three state-of-the-art algorithms from the most recent RL literature.","tags":[],"title":"NEAT for Large-Scale Reinforcement Learning through Evolutionary Feature Learning and Policy Gradient Search","type":"publication"},{"authors":["Will Hardwick-Smith","\u003cb\u003eYiming Peng\u003c/b\u003e","Gang Chen","Yi Mei","Mengjie Zhang"],"categories":null,"content":"","date":1484621910,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484621910,"objectID":"e6cc935999eaeec6ebe9c939dbec2683","permalink":"yimingpeng.github.io/publication/conference/c-paper-8/","publishdate":"2017-01-17T15:58:30+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-8/","section":"publication","summary":"NeuroEvolution of Augmenting Topologies (NEAT) has been successfully applied to intelligent gameplay. To further improve its effectiveness, a key technique is to reuse the knowledge learned from source gameplay tasks to boost performance on target gameplay tasks. We consider this as a Transfer Learning (TL) problem. However, Artificial Neural Networks (ANNs) evolved by NEAT are usually unnecessarily complicated, which may affect their transferability. To address this issue, we will investigate in this paper the capability of Phased Searching (PS) methods for controlling ANNs’ complexity while maintaining their effectiveness. By doing so, we can obtain more transferable ANNs. Furthermore, we will propose a new Power-Law Ranking Probability based PS (PLPS) method to more effectively control the randomness during the simplification phase. Several recent PS methods as well as our PLPS have been evaluated on four carefully-designed TL experiments. Results show clearly that NEAT can evolve more transferable and structurally simple ANNs with the help of PS methods, in particular PLPS.","tags":[],"title":"Evolving Transferable Artificial Neural Networks for Gameplay Tasks via NEAT with Phased Searching","type":"publication"},{"authors":["\u003cb\u003eYiming Peng\u003c/b\u003e","Gang Chen","Mengjie Zhang"],"categories":null,"content":"","date":1484620009,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484620009,"objectID":"10e482dcaad42ba9e4b4f06604c2b267","permalink":"yimingpeng.github.io/publication/conference/c-paper-5/","publishdate":"2017-01-17T15:26:49+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-5/","section":"publication","summary":"Actor-Critic algorithms have been increasingly re- searched for tackling challenging reinforcement learning problems. These algorithms are usually composed of two distinct learning processes, namely actor (a.k.a, policy) learning and critic (a.k.a, value function) learning. Actor learning is heavily dependent on critic learning; particularly unreliable critic learning due to its divergence can significantly affect the effectiveness of actor-critic algorithms. To address this issue, many successful algorithms have been developed recently with the aim of im- proving the accuracy of value function approximation. However, these algorithms introduce extra complexities to the learning process and may actually increase the difficulty for effective learning. Thus, in this research, we consider a simpler approach to improving the critic learning reliability. This approach requires us to seamlessly integrate an adapted Sandpile Model with the critic learning process so as to achieve desirable self-organizing property for reliable critic learning. Following this approach, we propose a new actor-critic learning algorithm. Its effectiveness and learning reliability have been further evaluated experimentally. As strongly demonstrated in the experiment results, our new algorithm can perform much better than traditional actor-critic algorithms. Meanwhile, correlation analysis further suggests that a strong correlation exists in between learning reliability and effectiveness. This finding may be important for future development of powerful reinforcement learning algorithms.","tags":[],"title":"A Sandpile Model for Reliable Actor-Critic Reinforcement Learning","type":"publication"},{"authors":["\u003cb\u003eYiming Peng\u003c/b\u003e","Gang Chen","Mengjie Zhang","Yi Mei"],"categories":null,"content":"","date":1484619753,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484619753,"objectID":"11b4cc67f49ec9eafb0153648f339135","permalink":"yimingpeng.github.io/publication/conference/c-paper-4/","publishdate":"2017-01-17T15:22:33+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-4/","section":"publication","summary":"To improve the effectiveness of commonly used Policy Gradient Search (PGS) algorithms for Reinforcement Learning (RL), many existing works considered the importance of extracting useful state features from raw environment inputs. However, these works only studied the feature extraction process, but the learned features have not been demonstrated to improve reinforcement learning performance. In this paper, we consider NeuroEvolution of Augmenting Topology (NEAT) for automated feature extraction, as it can evolve Neural Networks with suitable topologies that can help extract useful features. Following this idea, we develop a new algorithm called NEAT with Regular Actor Critic for Policy Gradient Search, which integrates a popular Actor-Critic PGS algorithm (i.e., Regular Actor-Critic) with NEAT based feature extraction. The algorithm manages to learn useful state features as well as good policies to tackle complex RL problems. The results on benchmark problems confirm that our proposed algorithm is significantly more effective than NEAT in terms of learning performance, and that the learned features by our proposed algorithm on one learning problem can maintain the effectiveness while it is used with RAC on another related learning problem.","tags":[],"title":"Effective Policy Gradient Search for Reinforcement Learning through NEAT based Feature Extraction","type":"publication"},{"authors":["\u003cb\u003eYiming Peng\u003c/b\u003e","Gang Chen","Scott Holdaway","Yi Mei","Mengjie Zhang"],"categories":null,"content":"","date":1484619401,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484619401,"objectID":"772bc4f495146d5f27ffc4ff00ae97f6","permalink":"yimingpeng.github.io/publication/conference/c-paper-3/","publishdate":"2017-01-17T15:16:41+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-3/","section":"publication","summary":"","tags":[],"title":"Automated State Feature Learning for Actor-Critic Reinforcement Learning through NEAT","type":"publication"},{"authors":["\u003cb\u003eYiming Peng\u003c/b\u003e","Gang Chen","Mengjie Zhang"],"categories":null,"content":"","date":1452996487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1452996487,"objectID":"01d3da6f5f67e7cf42541ffd4c9ecdea","permalink":"yimingpeng.github.io/publication/conference/c-paper-2/","publishdate":"2016-01-17T15:08:07+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-2/","section":"publication","summary":"Reinforcement learning aims at solving stochastic sequential decision making problems through direct trial-and-error interactions with the learning environment. In this paper, we will develop generalized compatible features to approximate value functions for reliable Reinforcement Learning. Further guided by an Actor-Critic Reinforcement Learning paradigm, we will also develop a generalized updating rule for policy gradient search in order to constantly improve learning performance. Our new updating rule has been examined on several benchmark learning problems. The experimental results on two problems will be reported specifically in this paper. Our results show that, under suitable generalization of the updating rule, the learning performance and reliability can be noticeably improved.","tags":[],"title":"Generalized Compatible Function Approximation for Policy Gradient Search","type":"publication"},{"authors":["Shaoning Pang","\u003cb\u003eYiming Peng\u003c/b\u003e","Tao Ban","Daisuke Inoue","Abdolhossein Sarrafzadeh"],"categories":null,"content":"","date":1421464568,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1421464568,"objectID":"6e83865613e0edd8c11256af909981cf","permalink":"yimingpeng.github.io/publication/conference/c-paper-12/","publishdate":"2015-01-17T16:16:08+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-12/","section":"publication","summary":"","tags":[],"title":"A Federated Online Network Traffic Analysis Engine for Cybersecurity.","type":"publication"},{"authors":["\u003cb\u003eYiming Peng\u003c/b\u003e","Shaoning Pang","Gang Chen","Abdolhossein Sarrafzadeh","Tao Ban","Daisuke Inoue"],"categories":null,"content":"","date":1358386425,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358386425,"objectID":"56ae86ed287941b42bcfceb72dca3ed5","permalink":"yimingpeng.github.io/publication/conference/c-paper-1/","publishdate":"2013-01-17T14:33:45+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-1/","section":"publication","summary":"Training data in real world is often presented in random chunks. Yet existing sequential Incremental IDR/QR LDA (s-QR/IncLDA) can only process data one sample after another. This paper proposes a constructive chunk Incremental IDR/QR LDA (c-QR/IncLDA) for multiple data samples incremental learning. Given a chunk of s samples for incremental learning, the proposed c-QR/IncLDA increments current discriminant model , by implementing computation on the compressed the residue matrix ∆ ∈ Rd×η, instead of the entire incoming data chunk Xf ∈ Rd×s, where η ≤ s holds. Meanwhile, we derive a more accurate reduced within-class scatter matrix W to minimize the discriminative information loss at every incremental learning cycle. It is noted that the computational complexity of c-QR/IncLDA can be more expensive than s-QR/IncLDA for single sample processing. However, for multiple samples processing, the computational efficiency of c-QR/IncLDA deterministically surpasses s-QR/IncLDA when the chunk size is large, i.e., s \u001d η holds. Moreover, experiments evaluation shows that the proposed c-QR/IncLDA can achieve an accuracy level that is competitive to batch QR/LDA and is consistently higher than s-QR/IncLDA.","tags":[],"title":"Chunk incremental IDR/QR LDA learning","type":"publication"},{"authors":["Anthony Lai","Lei Song","\u003cb\u003eYiming Peng\u003c/b\u003e","Peter Zhang","Qili Wang","Shaoning Pang"],"categories":null,"content":"","date":1326768788,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1326768788,"objectID":"f57c93f7b7c8bef559e6bd28c90864a9","permalink":"yimingpeng.github.io/publication/conference/c-paper-7/","publishdate":"2012-01-17T15:53:08+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-7/","section":"publication","summary":"This paper presented the relationship between the world price of crude oil and oil related stocks over 2011:4-2011:6 is analysed by using a graphical computational correlation analysis method. The Operation Neptune Spear happened in 2011:5 may change nature of the price connection between oil and stock, we evaluate and rank the im- pact of crude oil for the period before and after the event respectively. Over the statistical results, we find that graphic correlation is superior to typical point-to-point distance calculation for correlation analysis; and we discover stock ","tags":[],"title":"Exploring Crude Oil Impacts to Oil Stocks through Graphical Computational Correlation Analysis","type":"publication"}]