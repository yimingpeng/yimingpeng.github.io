[{"authors":["Yiming Peng","Gang Chen","Mengjie Zhang"],"categories":null,"content":"","date":1547687956,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547687956,"objectID":"b7b0dc7a7566c8a1e9d264c9f5edec68","permalink":"yimingpeng.github.io/publication/journal-paper-1/","publishdate":"2019-01-17T14:19:16+13:00","relpermalink":"yimingpeng.github.io/publication/journal-paper-1/","section":"publication","summary":"","tags":[],"title":"Proximal Evolutionary Strategies: Achieving State-of-the-art Deep Reinforcement Learning through Evolutionary Policy Optimization","type":"publication"},{"authors":["Yiming Peng","Gang Chen","Harman Singh","Mengjie Zhang"],"categories":null,"content":"","date":1516156231,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516156231,"objectID":"5448a64f967b54a5808be353a5e447c2","permalink":"yimingpeng.github.io/publication/conference/c-paper-6/","publishdate":"2018-01-17T15:30:31+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-6/","section":"publication","summary":"NeuroEvolution of Augmenting Topology (NEAT) is one of the most successful algorithms for solving traditional reinforcement learning (RL) tasks such as pole-balancing. However, the algorithm faces serious challenges while tackling problems with large state spaces, particularly the Atari game playing tasks. This is due to the major flaw that NEAT aims at evolving a single neural network (NN) that must be able to simultaneously extract high-level state features and select action outputs. However such complicated NNs cannot be easily evolved directly through NEAT. To address this issue, we propose a new reinforcement learning scheme based on NEAT with two key technical advancements: (1) a new three-stage learning scheme is introduced to clearly separate feature learning and policy learning to allow effective knowledge sharing and learning across multiple agents; (2) various policy gradient search algorithms can be seamlessly integrated with NEAT for training policy networks with deep structures to achieve effective and sample efficient RL. Experiments on several Atari games confirm that our new learning scheme can be more effective and has higher sample efficiency than NEAT and three state-of-the-art algorithms from the most recent RL literature.","tags":[],"title":"NEAT for Large-Scale Reinforcement Learning through Evolutionary Feature Learning and Policy Gradient Search","type":"publication"},{"authors":["Yiming Peng","Gang Chen","Mengjie Zhang"],"categories":null,"content":"","date":1484620009,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484620009,"objectID":"10e482dcaad42ba9e4b4f06604c2b267","permalink":"yimingpeng.github.io/publication/conference/c-paper-5/","publishdate":"2017-01-17T15:26:49+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-5/","section":"publication","summary":"Actor-Critic algorithms have been increasingly re- searched for tackling challenging reinforcement learning problems. These algorithms are usually composed of two distinct learning processes, namely actor (a.k.a, policy) learning and critic (a.k.a, value function) learning. Actor learning is heavily dependent on critic learning; particularly unreliable critic learning due to its divergence can significantly affect the effectiveness of actor-critic algorithms. To address this issue, many successful algorithms have been developed recently with the aim of im- proving the accuracy of value function approximation. However, these algorithms introduce extra complexities to the learning process and may actually increase the difficulty for effective learning. Thus, in this research, we consider a simpler approach to improving the critic learning reliability. This approach requires us to seamlessly integrate an adapted Sandpile Model with the critic learning process so as to achieve desirable self-organizing property for reliable critic learning. Following this approach, we propose a new actor-critic learning algorithm. Its effectiveness and learning reliability have been further evaluated experimentally. As strongly demonstrated in the experiment results, our new algorithm can perform much better than traditional actor-critic algorithms. Meanwhile, correlation analysis further suggests that a strong correlation exists in between learning reliability and effectiveness. This finding may be important for future development of powerful reinforcement learning algorithms.","tags":[],"title":"A Sandpile Model for Reliable Actor-Critic Reinforcement Learning","type":"publication"},{"authors":["Yiming Peng","Gang Chen","Mengjie Zhang","Yi Mei"],"categories":null,"content":"","date":1484619753,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484619753,"objectID":"11b4cc67f49ec9eafb0153648f339135","permalink":"yimingpeng.github.io/publication/conference/c-paper-4/","publishdate":"2017-01-17T15:22:33+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-4/","section":"publication","summary":"To improve the effectiveness of commonly used Policy Gradient Search (PGS) algorithms for Reinforcement Learning (RL), many existing works considered the importance of extracting useful state features from raw environment inputs. However, these works only studied the feature extraction process, but the learned features have not been demonstrated to improve reinforcement learning performance. In this paper, we consider NeuroEvolution of Augmenting Topology (NEAT) for automated feature extraction, as it can evolve Neural Networks with suitable topologies that can help extract useful features. Following this idea, we develop a new algorithm called NEAT with Regular Actor Critic for Policy Gradient Search, which integrates a popular Actor-Critic PGS algorithm (i.e., Regular Actor-Critic) with NEAT based feature extraction. The algorithm manages to learn useful state features as well as good policies to tackle complex RL problems. The results on benchmark problems confirm that our proposed algorithm is significantly more effective than NEAT in terms of learning performance, and that the learned features by our proposed algorithm on one learning problem can maintain the effectiveness while it is used with RAC on another related learning problem.","tags":[],"title":"Effective Policy Gradient Search for Reinforcement Learning through NEAT based Feature Extraction","type":"publication"},{"authors":["Yiming Peng","Gang Chen","Scott Holdaway","Yi Mei","Mengjie Zhang"],"categories":null,"content":"","date":1484619401,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484619401,"objectID":"772bc4f495146d5f27ffc4ff00ae97f6","permalink":"yimingpeng.github.io/publication/conference/c-paper-3/","publishdate":"2017-01-17T15:16:41+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-3/","section":"publication","summary":"","tags":[],"title":"Automated State Feature Learning for Actor-Critic Reinforcement Learning through NEAT","type":"publication"},{"authors":["Yiming Peng","Gang Chen","Mengjie Zhang"],"categories":null,"content":"","date":1452996487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1452996487,"objectID":"01d3da6f5f67e7cf42541ffd4c9ecdea","permalink":"yimingpeng.github.io/publication/conference/c-paper-2/","publishdate":"2016-01-17T15:08:07+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-2/","section":"publication","summary":"Reinforcement learning aims at solving stochastic sequential decision making problems through direct trial-and-error interactions with the learning environment. In this paper, we will develop generalized compatible features to approximate value functions for reliable Reinforcement Learning. Further guided by an Actor-Critic Reinforcement Learning paradigm, we will also develop a generalized updating rule for policy gradient search in order to constantly improve learning performance. Our new updating rule has been examined on several benchmark learning problems. The experimental results on two problems will be reported specifically in this paper. Our results show that, under suitable generalization of the updating rule, the learning performance and reliability can be noticeably improved.","tags":[],"title":"Generalized Compatible Function Approximation for Policy Gradient Search","type":"publication"},{"authors":["Yiming Peng","Shaoning Pang","Gang Chen","Abdolhossein Sarrafzadeh","Tao Ban","Daisuke Inoue"],"categories":null,"content":"","date":1358386425,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358386425,"objectID":"56ae86ed287941b42bcfceb72dca3ed5","permalink":"yimingpeng.github.io/publication/conference/c-paper-1/","publishdate":"2013-01-17T14:33:45+13:00","relpermalink":"yimingpeng.github.io/publication/conference/c-paper-1/","section":"publication","summary":"Training data in real world is often presented in random chunks. Yet existing sequential Incremental IDR/QR LDA (s-QR/IncLDA) can only process data one sample after another. This paper proposes a constructive chunk Incremental IDR/QR LDA (c-QR/IncLDA) for multiple data samples incremental learning. Given a chunk of s samples for incremental learning, the proposed c-QR/IncLDA increments current discriminant model , by implementing computation on the compressed the residue matrix ∆ ∈ Rd×η, instead of the entire incoming data chunk Xf ∈ Rd×s, where η ≤ s holds. Meanwhile, we derive a more accurate reduced within-class scatter matrix W to minimize the discriminative information loss at every incremental learning cycle. It is noted that the computational complexity of c-QR/IncLDA can be more expensive than s-QR/IncLDA for single sample processing. However, for multiple samples processing, the computational efficiency of c-QR/IncLDA deterministically surpasses s-QR/IncLDA when the chunk size is large, i.e., s \u001d η holds. Moreover, experiments evaluation shows that the proposed c-QR/IncLDA can achieve an accuracy level that is competitive to batch QR/LDA and is consistently higher than s-QR/IncLDA.","tags":[],"title":"Chunk incremental IDR/QR LDA learning","type":"publication"}]